{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pre-Trained Embeddings into Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(inputData):\n",
    "    dataset = pd.read_csv(inputData)\n",
    "    dataset = dataset[dataset['grades'] != 0] # remove entries with 0 grades\n",
    "    dataset['replaced_sentence'] = \"\" #replace word in news headline\n",
    "    storage_array = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        new = re.sub('<.*/>', row['edit'], row['original'], flags=re.DOTALL)\n",
    "        storage_array.append(new)\n",
    "    dataset['replaced_sentence'] = storage_array\n",
    "    # make characters lowercase\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    stop = stopwords.words('english') #remove stop words\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].str.replace('[^\\w\\s]','')\n",
    "    frequent_words = pd.Series(' '.join(dataset['replaced_sentence']).split()).value_counts()[:10] # remove common words\n",
    "    words_to_remove = ['s', 'nt', 'nd']\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words_to_remove))\n",
    "    \n",
    "    # new\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x in embeddings_dict))\n",
    "    \n",
    "    rare = pd.Series(' '.join(dataset['replaced_sentence']).split()).value_counts()[-10:]\n",
    "    rare = list(rare.index) #remove rare words\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "    dataset['replaced_sentence'] = dataset['replaced_sentence'].str.replace('\\d+', '')\n",
    "    dataset = dataset.drop(['original', 'edit', 'grades'], axis=1) #drop unnecessary cols\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>replaced_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530</td>\n",
       "      <td>0.2</td>\n",
       "      <td>france hunting citizens joined twins without t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034</td>\n",
       "      <td>1.6</td>\n",
       "      <td>pentagon claims  increase russian trolls bowli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>iceland pm calls snap vote pedophile furor cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>0.4</td>\n",
       "      <td>apparent first iran israel slap militarily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8832</td>\n",
       "      <td>1.2</td>\n",
       "      <td>sounds trump made speech congress one chart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  meanGrade                                  replaced_sentence\n",
       "0  14530        0.2  france hunting citizens joined twins without t...\n",
       "1  13034        1.6  pentagon claims  increase russian trolls bowli...\n",
       "2   8731        1.0  iceland pm calls snap vote pedophile furor cra...\n",
       "3     76        0.4         apparent first iran israel slap militarily\n",
       "5   8832        1.2        sounds trump made speech congress one chart"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = data_preprocessing(\"train.csv\")\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data = spark.createDataFrame(processed_data)\\\n",
    "                .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_sentence = rdd_data.map(lambda input: (input.id, input.replaced_sentence.split()))\n",
    "#id_with_sentence.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize assigns words to their 50-d word vectors and divides by the number of words in the sentence... this is part of the process for getting the averages for the sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    result = []\n",
    "    for x in input[1]: \n",
    "        length = len(input[1])                                     # gets number of words in sentence\n",
    "        embedding = []\n",
    "        for i in list(embeddings_dict[x]):\n",
    "            embedding.append(float(i / length))\n",
    "        result.append((input[0],embedding))\n",
    "    return result\n",
    "\n",
    "id_with_word = id_with_sentence.flatMap(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_with_word.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def myReducer(x,y):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for i in range(0, len(x)):\n",
    "        result.append(x[i] + y[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_groups = id_with_word.reduceByKey(myReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID GROUPS IS THE FINAL PRODUCT\n",
    "#id_groups.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Ratings Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_grade = rdd_data.map(lambda input: (input.id, input.meanGrade))\n",
    "#id_with_grade.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|grade|identif|\n",
      "+-----+-------+\n",
      "|  0.2|  14530|\n",
      "|  1.6|  13034|\n",
      "|  1.0|   8731|\n",
      "|  0.4|     76|\n",
      "|  1.2|   8832|\n",
      "+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "grades = id_with_grade.map(lambda x: Row(identif=x[0], grade=x[1]))\n",
    "schemaGrades = sqlContext.createDataFrame(grades)\n",
    "schemaGrades.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|               embed|identif|\n",
      "+--------------------+-------+\n",
      "|[0.03392983662585...|     76|\n",
      "|[0.44911583544065...|     68|\n",
      "|[-0.1882088308533...|   4136|\n",
      "|[-0.5004666546980...|   7080|\n",
      "|[0.31943100318312...|  10224|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = id_groups.map(lambda x: Row(identif=x[0], embed=x[1]))\n",
    "#sentence_embeddings.collect()[0]\n",
    "schemaEmbed = sqlContext.createDataFrame(sentence_embeddings)\n",
    "schemaEmbed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|grade|               embed|\n",
      "+-----+--------------------+\n",
      "|  1.2|[-0.0015900052256...|\n",
      "|  0.8|[-0.0339350104331...|\n",
      "|  1.2|[-0.2413649982255...|\n",
      "|  2.2|[0.05566571706107...|\n",
      "|  1.2|[-0.0196700021624...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = schemaGrades.join(schemaEmbed,on='identif',how='left').select(['grade','embed'])\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|grade|               embed|\n",
      "+-----+--------------------+\n",
      "|  1.2|[-0.0015900052256...|\n",
      "|  0.8|[-0.0339350104331...|\n",
      "|  1.2|[-0.2413649982255...|\n",
      "|  2.2|[0.05566571706107...|\n",
      "|  1.2|[-0.0196700021624...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## see https://stackoverflow.com/questions/39025707/how-to-convert-arraytype-to-densevector-in-pyspark-dataframe\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "grade_rdd = df.rdd.map(lambda row:row[0])\n",
    "embed_rdd = df.rdd.map(lambda row:row[1])\n",
    "new_df = grade_rdd.zip(embed_rdd.map(lambda x:Vectors.dense(x))).toDF(schema=['grade','embed'])\n",
    "\n",
    "new_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = new_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|grade|               embed|\n",
      "+-----+--------------------+\n",
      "|  0.2|[0.00719059556722...|\n",
      "|  0.2|[0.39878216525539...|\n",
      "|  0.4|[-0.1359110943973...|\n",
      "|  0.4|[-0.1213597480673...|\n",
      "|  0.4|[0.05640549398958...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
